import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
import os
import logging
import base64
import asyncio
import sys
import time
import threading
from datetime import datetime, timedelta
from simple_salesforce import Salesforce

warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Config:
    RANDOM_STATE = 42
    TEST_SIZE = 0.2
    MODEL_TYPE = 'random_forest'
   
    RF_N_ESTIMATORS = 100
    RF_MAX_DEPTH = 10
    RF_MIN_SAMPLES_SPLIT = 20
    RF_MIN_SAMPLES_LEAF = 10
   
    SCORE_THRESHOLDS = {'hot': 80, 'warm': 60, 'cool': 40, 'cold': 0}
    
    # Batch processing settings
    BATCH_SIZE = 200
    DAILY_UPDATE_HOUR = 22  # 10 PM
    OLD_LEAD_DAYS_THRESHOLD = 7  # Process leads older than 7 days

class SalesforceIntegrator:
    def __init__(self):
        self.sf = None
        self._authenticate()
    
    def _authenticate(self):
        """Authenticate to Salesforce using env vars."""
        username = 'kshitijakapadane@03.com'
        password = '03Sep2024'
        security_token = '9PZPUdlPAmibrmf5rnlUCSBFG'
        domain = 'login'
        print(f"Username: {username}")
        print(f"Domain: {domain}")  
        
        if not all([username, password, security_token]):
            raise ValueError("Salesforce credentials missing.")
        
        try:
            self.sf = Salesforce(username=username, password=password, 
                               security_token=security_token, domain=domain)
            logger.info("Authenticated to Salesforce successfully.")
        except Exception as e:
            logger.error(f"Authentication failed: {e}")
            logger.warning("Continuing without Salesforce integration.")
    
    def fetch_leads_from_salesforce(self, limit=10):
        """Fetch real leads from Salesforce with the specified fields."""
        if not self.sf:
            logger.warning("Salesforce not connected. Cannot fetch leads.")
            return pd.DataFrame()
            
        try:
            query = f"""
            SELECT 
                CreatedDate,
                Id,
                Name
            FROM Lead 
            LIMIT {limit}
            """
            
            print(f"Fetching {limit} leads from Salesforce...")
            results = self.sf.query_all(query)
            
            if results['totalSize'] > 0:
                print(f" Successfully fetched {results['totalSize']} leads from Salesforce!")
                
                leads_data = []
                for record in results['records']:
                    lead_info = {
                        'Id': record.get('Id'),
                        'Name': record.get('Name'),
                        'CreatedDate': record.get('CreatedDate')
                    }
                    leads_data.append(lead_info)
                
                df = pd.DataFrame(leads_data)
                return df
            else:
                print(" No leads found in Salesforce.")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"Failed to fetch leads from Salesforce: {e}")
            return pd.DataFrame()
    
    def fetch_old_leads_for_batch_processing(self, days_old=7, batch_size=200):
        """Fetch older leads for daily batch processing with engagement data."""
        if not self.sf:
            logger.warning("Salesforce not connected. Cannot fetch old leads.")
            return pd.DataFrame()
            
        try:
            # Calculate date threshold
            cutoff_date = (datetime.now() - timedelta(days=days_old)).strftime('%Y-%m-%dT%H:%M:%SZ')
            
            query = f"""
            SELECT 
                Id,
                Name,
                Email,
                Company,
                LeadSource,
                Industry,
                CreatedDate,
                LastModifiedDate,
                Website,
                Description
            FROM Lead 
            WHERE CreatedDate < {cutoff_date}
            ORDER BY CreatedDate DESC
            LIMIT {batch_size}
            """
            
            logger.info(f"Fetching {batch_size} old leads (created before {cutoff_date})...")
            results = self.sf.query_all(query)
            
            if results['totalSize'] > 0:
                leads_data = []
                for record in results['records']:
                    lead_info = {
                        'Id': record.get('Id'),
                        'Name': record.get('Name'),
                        'Email': record.get('Email'),
                        'Company': record.get('Company'),
                        'lead_source': record.get('LeadSource'),
                        'industry': record.get('Industry'),
                        'CreatedDate': record.get('CreatedDate'),
                        'LastModifiedDate': record.get('LastModifiedDate'),
                        'Website': record.get('Website'),
                        'Description': record.get('Description')
                    }
                    leads_data.append(lead_info)
                
                df = pd.DataFrame(leads_data)
                logger.info(f"Fetched {len(df)} old leads for batch processing")
                return df
            else:
                logger.info("No old leads found for batch processing.")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"Failed to fetch old leads: {e}")
            return pd.DataFrame()
    
    def update_lead(self, lead_id, updates):
        """Update a single Lead record by ID."""
        if not self.sf:
            logger.warning("Salesforce not connected. Skipping update.")
            return None
            
        try:
            sf_updates = {
                'AI_Score__c': str(updates.get('score')) if updates.get('score') is not None else None,
                'AI_Category__c': str(updates.get('category')) if updates.get('category') is not None else None,
                'AI_Priority__c': str(updates.get('priority')) if updates.get('priority') is not None else None,
                'AI_Recommendation__c': str(updates.get('recommendation')) if updates.get('recommendation') is not None else None,
                'AI_Last_Updated__c': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            sf_updates = {k: v for k, v in sf_updates.items() if v is not None}
            
            lead_id_str = str(lead_id)
            
            result = self.sf.Lead.update(lead_id_str, sf_updates)
            logger.info(f"Updated Lead {lead_id_str}")
            return result
        except Exception as e:
            logger.error(f"Failed to update Lead {lead_id}: {e}")
            return None
    
    def batch_update_leads(self, updates_list):
        """Update multiple leads in batch."""
        if not self.sf:
            logger.warning("Salesforce not connected. Skipping batch update.")
            return None
            
        try:
            successful_updates = 0
            for update_data in updates_list:
                lead_id = update_data.get('lead_id')
                updates = update_data.get('updates')
                
                if lead_id and updates:
                    result = self.update_lead(lead_id, updates)
                    if result:
                        successful_updates += 1
            
            logger.info(f"Batch update completed: {successful_updates}/{len(updates_list)} successful")
            return successful_updates
        except Exception as e:
            logger.error(f"Batch update failed: {e}")
            return None

class DataSimulator:
    def __init__(self, random_state=Config.RANDOM_STATE):
        self.random_state = random_state
        np.random.seed(random_state)
   
    def generate_sample_data(self, n_samples=50):
        print("Generating sample lead data...")
        data = {
            'lead_id': [f"{i+1}" for i in range(n_samples)],
            'lead_source': np.random.choice(
                ['Website', 'Email', 'Social Media', 'Referral', 'Cold Call', 'Paid Search'],
                n_samples,
                p=[0.3, 0.2, 0.15, 0.15, 0.1, 0.1]
            ),
            'company_size': np.random.choice(
                ['Small', 'Medium', 'Large', 'Enterprise'],
                n_samples,
                p=[0.4, 0.3, 0.2, 0.1]
            ),
            'industry': np.random.choice(
                ['Technology', 'Healthcare', 'Finance', 'Education', 'Manufacturing', 'Retail'],
                n_samples,
                p=[0.25, 0.2, 0.15, 0.15, 0.15, 0.1]
            ),
            'country': np.random.choice(
                ['USA', 'UK', 'Canada', 'Germany', 'Australia', 'Other'],
                n_samples,
                p=[0.5, 0.15, 0.1, 0.1, 0.05, 0.1]
            ),
            'website_visits': np.random.poisson(5, n_samples),
            'email_opens': np.random.poisson(3, n_samples),
            'time_on_site': np.random.exponential(300, n_samples),
            'pages_viewed': np.random.poisson(8, n_samples),
            'form_submissions': np.random.poisson(1, n_samples),
            'email_click_rate': np.random.beta(2, 5, n_samples),
            'days_since_created': np.random.exponential(30, n_samples),
            'has_demo_requested': np.random.binomial(1, 0.2, n_samples),
            'content_downloads': np.random.poisson(0.5, n_samples),
        }
       
        df = pd.DataFrame(data)
        df['converted'] = self._generate_target_variable(df)
       
        print(f"Generated {len(df)} sample leads")
        print(f"Conversion rate: {df['converted'].mean():.2%}")
        return df
   
    def _generate_target_variable(self, df):
        conversion_probability = (
            (df['website_visits'] * 0.1) +
            (df['email_opens'] * 0.05) +
            (df['pages_viewed'] * 0.08) +
            (df['form_submissions'] * 0.3) +
            (df['email_click_rate'] * 2) +
            (df['time_on_site'] * 0.001) +
            (df['has_demo_requested'] * 0.4) +
            (df['content_downloads'] * 0.2) -
            (df['days_since_created'] * 0.01)
        )
       
        industry_bonus = {
            'Technology': 0.3, 'Finance': 0.2, 'Healthcare': 0.1,
            'Manufacturing': 0.0, 'Education': -0.1, 'Retail': 0.0
        }
        conversion_probability += df['industry'].map(industry_bonus)
       
        source_bonus = {
            'Referral': 0.4, 'Website': 0.2, 'Paid Search': 0.1,
            'Email': 0.0, 'Social Media': -0.1, 'Cold Call': -0.2
        }
        conversion_probability += df['lead_source'].map(source_bonus)
       
        conversion_probability += np.random.normal(0, 0.3, len(df))
        return (conversion_probability > conversion_probability.median()).astype(int)
   
    def generate_new_leads(self, n_leads=10):
        df = self.generate_sample_data(n_leads)
        return df.drop('converted', axis=1)

class LeadScoringAI:
    def __init__(self, model_type=Config.MODEL_TYPE):
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self.data_simulator = DataSimulator()
   
    def preprocess_data(self, df, is_training=True):
        processed_df = df.copy()
       
        numeric_columns = ['website_visits', 'email_opens', 'time_on_site', 'pages_viewed', 
                          'form_submissions', 'email_click_rate', 'days_since_created', 
                          'has_demo_requested', 'content_downloads']
        
        for col in numeric_columns:
            if col in processed_df.columns:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0)
       
        processed_df['engagement_score'] = (
            processed_df['website_visits'] * 0.3 +
            processed_df['email_opens'] * 0.2 +
            processed_df['pages_viewed'] * 0.5 +
            processed_df['content_downloads'] * 0.5
        )
       
        processed_df['conversion_velocity'] = (
            processed_df['form_submissions'] / (processed_df['days_since_created'] + 1)
        )
       
        processed_df['digital_footprint'] = (
            processed_df['website_visits'] +
            processed_df['pages_viewed'] +
            processed_df['form_submissions'] * 3
        )
       
        categorical_cols = ['lead_source', 'company_size', 'industry', 'country']
        available_categorical_cols = [col for col in categorical_cols if col in processed_df.columns]
        
        if available_categorical_cols:
            processed_df = pd.get_dummies(processed_df, columns=available_categorical_cols, drop_first=True)
       
        if is_training:
            self.feature_columns = [col for col in processed_df.columns if col != 'converted']
       
        if not is_training and self.feature_columns is not None:
            for col in self.feature_columns:
                if col not in processed_df.columns:
                    processed_df[col] = 0
            processed_df = processed_df[self.feature_columns]
       
        return processed_df
   
    def plot_feature_importance(self, feature_names, top_n=15):
        if hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
        else:
            importance = np.abs(self.model.coef_[0])
       
        indices = np.argsort(importance)[::-1]
       
        plt.figure(figsize=(12, 8))
        plt.title("Feature Importance - Lead Scoring Model", fontsize=16, fontweight='bold')
        bars = plt.barh(range(min(top_n, len(indices))),
                       importance[indices][:top_n][::-1],
                       color='skyblue', edgecolor='black')
       
        plt.yticks(range(min(top_n, len(indices))),
                  [feature_names[i] for i in indices[:top_n]][::-1], fontsize=12)
        plt.xlabel('Importance Score', fontsize=12)
        plt.grid(axis='x', alpha=0.3)
       
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{width:.3f}', ha='left', va='center', fontsize=10)
       
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        plt.show()
   
    def plot_lead_score_distribution(self, scores):
        thresholds = Config.SCORE_THRESHOLDS
       
        plt.figure(figsize=(12, 6))
        colors = ['red', 'orange', 'lightblue', 'darkblue']
       
        n, bins, patches = plt.hist(scores, bins=50, alpha=0.7, edgecolor='black')
       
        for i, (patch, left_bin, right_bin) in enumerate(zip(patches, bins[:-1], bins[1:])):
            if right_bin <= thresholds['cool']:
                patch.set_facecolor(colors[3])
            elif right_bin <= thresholds['warm']:
                patch.set_facecolor(colors[2])
            elif right_bin <= thresholds['hot']:
                patch.set_facecolor(colors[1])
            else:
                patch.set_facecolor(colors[0])
       
        plt.title('Lead Score Distribution', fontsize=16, fontweight='bold')
        plt.xlabel('Lead Score', fontsize=12)
        plt.ylabel('Number of Leads', fontsize=12)
        plt.grid(True, alpha=0.3)
       
        for threshold_name, threshold_value in thresholds.items():
            if threshold_value > 0:
                plt.axvline(x=threshold_value, color='black', linestyle='--', alpha=0.7)
                plt.text(threshold_value + 1, plt.ylim()[1] * 0.9,
                        f'{threshold_name.title()}: {threshold_value}',
                        rotation=90, verticalalignment='top')
       
        plt.tight_layout()
        plt.savefig('lead_score_distribution.png')
        plt.show()
   
    def evaluate_model_performance(self, y_true, y_pred, y_pred_proba):
        print("=" * 60)
        print("LEAD SCORING MODEL EVALUATION")
        print("=" * 60)
       
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        print(f"ROC-AUC Score: {roc_auc:.4f}")
       
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred))
       
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Not Converted', 'Converted'],
                   yticklabels=['Not Converted', 'Converted'])
        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.savefig('confusion_matrix.png')
        plt.show()
       
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, marker='.')
        plt.title('Precision-Recall Curve')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.grid(True, alpha=0.3)
        plt.savefig('precision_recall.png')
        plt.show()
       
        return roc_auc
   
    def train(self, n_samples=1000, test_size=Config.TEST_SIZE):  
        print("Training Lead Scoring Model...")
       
        df = self.data_simulator.generate_sample_data(n_samples)
       
        processed_data = self.preprocess_data(df)
        X = processed_data.drop('converted', axis=1)
        y = processed_data['converted']
       
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=Config.RANDOM_STATE, stratify=y
        )
       
        X_train_values = X_train.values
        X_test_values = X_test.values
       
        X_train_scaled = self.scaler.fit_transform(X_train_values)
        X_test_scaled = self.scaler.transform(X_test_values)
       
        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=Config.RF_N_ESTIMATORS,
                max_depth=Config.RF_MAX_DEPTH,
                min_samples_split=Config.RF_MIN_SAMPLES_SPLIT,
                min_samples_leaf=Config.RF_MIN_SAMPLES_LEAF,
                random_state=Config.RANDOM_STATE
            )
        elif self.model_type == 'logistic_regression':
            self.model = LogisticRegression(random_state=Config.RANDOM_STATE, max_iter=1000)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
       
        self.model.fit(X_train_scaled, y_train)
        self.is_trained = True
       
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
       
        roc_auc = self.evaluate_model_performance(y_test, y_pred, y_pred_proba)
       
        if hasattr(self.model, 'feature_importances_'):
            self.plot_feature_importance(X.columns)
       
        print("Model training completed!")
        return roc_auc
   
    def predict_scores(self, lead_data):
        if not self.is_trained:
            raise ValueError("Model not trained. Please call train() first.")
       
        if isinstance(lead_data, dict):
            lead_df = pd.DataFrame([lead_data])
        elif isinstance(lead_data, list):
            lead_df = pd.DataFrame(lead_data)
        else:
            lead_df = lead_data.copy()
       
        lead_processed = self.preprocess_data(lead_df, is_training=False)
        lead_processed_values = lead_processed.values
        lead_scaled = self.scaler.transform(lead_processed_values)
        conversion_probabilities = self.model.predict_proba(lead_scaled)[:, 1]
        lead_scores = (conversion_probabilities * 100).astype(int)
       
        return lead_scores
   
    def categorize_leads(self, scores):
        thresholds = Config.SCORE_THRESHOLDS
        categories = []
        for score in scores:
            if score >= thresholds['hot']:
                categories.append("Hot")
            elif score >= thresholds['warm']:
                categories.append("Warm")
            elif score >= thresholds['cool']:
                categories.append("Cool")
            else:
                categories.append("Cold")
        return categories
   
    def analyze_leads(self, lead_data):
        if isinstance(lead_data, dict):
            lead_data = pd.DataFrame([lead_data])
        elif isinstance(lead_data, list):
            lead_data = pd.DataFrame(lead_data)
       
        scores = self.predict_scores(lead_data)
        categories = self.categorize_leads(scores)
       
        results = []
        for i, (score, category) in enumerate(zip(scores, categories)):
            lead_id = lead_data.iloc[i].get('Id') or lead_data.iloc[i].get('id') or lead_data.iloc[i].get('lead_id') or f"lead_{i+1}"
            result = {
                'lead_id': str(lead_id),  
                'score': int(score),      
                'category': str(category),
                'priority': self._get_priority_level(category),
                'recommendation': self._get_recommendation(category, score)
            }
            results.append(result)
       
        return pd.DataFrame(results)
   
    def _get_priority_level(self, category):
        priority_map = {'Hot': 'High', 'Warm': 'Medium', 'Cool': 'Low', 'Cold': 'Very Low'}
        return priority_map.get(category, 'Unknown')
   
    def _get_recommendation(self, category, score):
        recommendations = {
            'Hot': f"Immediate follow-up! Call within 24 hours. Score: {score}",
            'Warm': f"Schedule follow-up this week. Nurture with targeted content. Score: {score}",
            'Cool': f"Add to email nurture sequence. Monitor engagement. Score: {score}",
            'Cold': f"Low priority. Generic newsletter only. Re-engage later. Score: {score}"
        }
        return recommendations.get(category, "No recommendation available")
   
    def save_model(self, filepath='lead_scoring_model.pkl'):
        if not self.is_trained:
            raise ValueError("No trained model to save.")
       
        model_data = {
            'model': self.model,
            'feature_columns': self.feature_columns,
            'scaler': self.scaler,
            'config': {'thresholds': Config.SCORE_THRESHOLDS, 'model_type': Config.MODEL_TYPE}
        }
       
        joblib.dump(model_data, filepath)
        print(f"Model saved successfully to: {filepath}")
   
    def load_model(self, filepath='lead_scoring_model.pkl'):
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.feature_columns = model_data['feature_columns']
        self.scaler = model_data['scaler']
        self.is_trained = True
        print(f"Model loaded successfully from: {filepath}")
    
    def analyze_and_update_old_leads(self, old_leads_data):
        """Analyze old leads and return updates with change detection."""
        if not self.is_trained:
            raise ValueError("Model not trained. Please call train() first.")
        
        if old_leads_data.empty:
            return []
        
        # Generate enhanced data for old leads
        enhanced_leads = []
        for _, lead in old_leads_data.iterrows():
            enhanced_lead = self.enhance_old_lead_data(lead)
            enhanced_leads.append(enhanced_lead)
        
        enhanced_df = pd.DataFrame(enhanced_leads)
        
        # Score the leads
        scores = self.predict_scores(enhanced_df)
        categories = self.categorize_leads(scores)
        
        # Prepare updates
        updates_list = []
        for i, (lead, score, category) in enumerate(zip(old_leads_data.to_dict('records'), scores, categories)):
            update_data = {
                'lead_id': lead['Id'],
                'updates': {
                    'score': int(score),
                    'category': str(category),
                    'priority': self._get_priority_level(category),
                    'recommendation': self._get_recommendation(category, score)
                }
            }
            updates_list.append(update_data)
            
            logger.info(f"Old lead {lead['Name']} - Score: {score}, Category: {category}")
        
        return updates_list
    
    def enhance_old_lead_data(self, lead):
        """Enhance old lead data with simulated recent engagement."""
        base_data = {
            'Id': lead['Id'],
            'Name': lead['Name'],
            'Email': lead.get('Email', ''),
            'Company': lead.get('Company', ''),
            'lead_source': lead.get('lead_source') or np.random.choice(['Website', 'Email', 'Social Media', 'Referral']),
            'company_size': np.random.choice(['Small', 'Medium', 'Large', 'Enterprise'], p=[0.4, 0.3, 0.2, 0.1]),
            'industry': lead.get('industry') or np.random.choice(['Technology', 'Healthcare', 'Finance', 'Education', 'Manufacturing', 'Retail']),
            'country': 'USA'
        }
        
        # Simulate engagement data for old leads
        engagement_data = {
            'website_visits': np.random.randint(1, 25),
            'email_opens': np.random.randint(1, 20),
            'time_on_site': np.random.randint(100, 1200),
            'pages_viewed': np.random.randint(5, 30),
            'form_submissions': np.random.randint(0, 5),
            'email_click_rate': np.random.uniform(0.1, 0.9),
            'days_since_created': self._calculate_days_since_created(lead.get('CreatedDate')),
            'has_demo_requested': np.random.choice([0, 1], p=[0.8, 0.2]),
            'content_downloads': np.random.randint(0, 3)
        }
        
        return {**base_data, **engagement_data}
    
    def _calculate_days_since_created(self, created_date):
        """Calculate days since lead was created."""
        if created_date:
            if isinstance(created_date, str):
                created_date = datetime.strptime(created_date.split('T')[0], '%Y-%m-%d')
            return (datetime.now() - created_date).days
        return np.random.randint(1, 90)

class SalesforceEventSubscriber:
    def __init__(self, sf_integrator):
        self.sf_integrator = sf_integrator
        self.lead_scoring_ai = None
        self.last_processed_time = None
        
    def set_ai_model(self, ai_model):
        """Set the AI model for lead scoring"""
        self.lead_scoring_ai = ai_model
    
    def get_recent_leads(self, minutes=5):
        """Get leads created in the last X minutes"""
        try:
            if not self.sf_integrator.sf:
                return []
                
            time_threshold = (datetime.utcnow() - timedelta(minutes=minutes)).strftime('%Y-%m-%dT%H:%M:%SZ')
            
            query = f"""
            SELECT Id, Name, Email, Company, CreatedDate 
            FROM Lead 
            WHERE CreatedDate > {time_threshold}
            ORDER BY CreatedDate DESC
            """
            
            results = self.sf_integrator.sf.query_all(query)
            
            if results['totalSize'] > 0:
                leads = []
                for record in results['records']:
                    leads.append({
                        'Id': record['Id'],
                        'Name': record['Name'],
                        'Email': record.get('Email', ''),
                        'Company': record.get('Company', ''),
                        'CreatedDate': record['CreatedDate']
                    })
                return leads
            else:
                return []
                
        except Exception as e:
            logger.error(f"Error fetching recent leads: {e}")
            return []
    
    def process_new_lead(self, lead_data):
        """Process and score a new lead"""
        try:
            logger.info(f" Processing new lead: {lead_data['Name']} ({lead_data['Id']})")
            
            enhanced_lead_data = self.generate_lead_engagement_data(lead_data)
            
            results = self.lead_scoring_ai.analyze_leads([enhanced_lead_data])
            
            for _, result in results.iterrows():
                update_data = {
                    'score': result['score'],
                    'category': result['category'],
                    'priority': result['priority'],
                    'recommendation': result['recommendation']
                }
                
                self.sf_integrator.update_lead(lead_data['Id'], update_data)
                logger.info(f" Updated lead {lead_data['Name']} with score: {result['score']}")
                
        except Exception as e:
            logger.error(f" Error processing lead {lead_data['Id']}: {e}")
    
    def generate_lead_engagement_data(self, lead_data):
        """Generate realistic engagement data for new lead"""
        return {
            'Id': lead_data['Id'],
            'Name': lead_data['Name'],
            'Email': lead_data.get('Email', ''),
            'Company': lead_data.get('Company', ''),
            'lead_source': np.random.choice(['Website', 'Email', 'Social Media', 'Referral', 'Paid Search']),
            'company_size': np.random.choice(['Small', 'Medium', 'Large', 'Enterprise'], p=[0.4, 0.3, 0.2, 0.1]),
            'industry': np.random.choice(['Technology', 'Healthcare', 'Finance', 'Education', 'Manufacturing', 'Retail']),
            'country': 'USA',
            'website_visits': np.random.randint(1, 25),
            'email_opens': np.random.randint(1, 20),
            'time_on_site': np.random.randint(100, 1200),
            'pages_viewed': np.random.randint(5, 30),
            'form_submissions': np.random.randint(0, 5),
            'email_click_rate': np.random.uniform(0.1, 0.9),
            'days_since_created': 0,
            'has_demo_requested': np.random.choice([0, 1], p=[0.8, 0.2]),
            'content_downloads': np.random.randint(0, 3)
        }
    
    def process_old_leads_batch(self):
        """Process old leads in batch for daily updates."""
        try:
            logger.info("Starting daily batch processing for old leads...")
            
            # Fetch old leads
            old_leads_df = self.sf_integrator.fetch_old_leads_for_batch_processing(
                days_old=Config.OLD_LEAD_DAYS_THRESHOLD,
                batch_size=Config.BATCH_SIZE
            )
            
            if old_leads_df.empty:
                logger.info("No old leads found for batch processing.")
                return 0
            
            # Analyze and get updates
            updates_list = self.lead_scoring_ai.analyze_and_update_old_leads(old_leads_df)
            
            if not updates_list:
                logger.info("No updates needed for old leads.")
                return 0
            
            # Batch update
            successful_updates = self.sf_integrator.batch_update_leads(updates_list)
            
            logger.info(f"Daily batch processing completed: {successful_updates} leads updated")
            
            return successful_updates
            
        except Exception as e:
            logger.error(f"Error in batch processing old leads: {e}")
            return 0
    
    def schedule_daily_batch(self):
        """Schedule daily batch processing using threading.Timer"""
        try:
            now = datetime.now()
            target_time = now.replace(hour=Config.DAILY_UPDATE_HOUR, minute=0, second=0, microsecond=0)
            
            # If target time is in the past, schedule for tomorrow
            if now > target_time:
                target_time += timedelta(days=1)
            
            delay_seconds = (target_time - now).total_seconds()
            
            logger.info(f"Scheduling daily batch processing for {target_time} "
                       f"(in {delay_seconds/3600:.1f} hours)")
            
            # Create timer that will run once
            timer = threading.Timer(delay_seconds, self._run_daily_batch)
            timer.daemon = True
            timer.start()
            
        except Exception as e:
            logger.error(f"Error scheduling daily batch: {e}")
    
    def _run_daily_batch(self):
        """Run batch processing and reschedule for next day"""
        try:
            # Run the batch processing
            self.process_old_leads_batch()
            
            # Reschedule for next day
            self.schedule_daily_batch()
            
        except Exception as e:
            logger.error(f"Error in daily batch execution: {e}")
            # Still reschedule for next day even if this run failed
            self.schedule_daily_batch()
    
    async def start_polling(self, interval_seconds=30):
        """Start polling for new leads (async version)"""
        try:
            logger.info(" Starting lead polling service...")
            logger.info(f" Checking for new leads every {interval_seconds} seconds")
            logger.info(" The system will automatically score new leads!")
            logger.info(" Press Ctrl+C to stop\n")
            
            processed_leads = set()
            
            while True:
                try:
                    recent_leads = await asyncio.to_thread(self.get_recent_leads, minutes=10)
                    
                    new_leads_found = 0
                    
                    process_tasks = []
                    for lead in recent_leads:
                        lead_id = lead['Id']
                        
                        if lead_id in processed_leads:
                            continue
                        
                        task = asyncio.to_thread(self.process_new_lead, lead)
                        process_tasks.append(task)
                        processed_leads.add(lead_id)
                    
                    if process_tasks:
                        await asyncio.gather(*process_tasks, return_exceptions=True)
                        new_leads_found = len(process_tasks)
                    
                    if new_leads_found > 0:
                        logger.info(f" Processed {new_leads_found} new lead(s)")
                    
                    await asyncio.sleep(interval_seconds)
                    
                except KeyboardInterrupt:
                    logger.info(" Polling stopped by user")
                    break
                except Exception as e:
                    logger.error(f" Polling error: {e}")
                    await asyncio.sleep(interval_seconds)  
                    
        except Exception as e:
            logger.error(f" Polling service error: {e}")

def main_automated():
    """Automated version that polls for new leads AND processes old leads daily"""
    print(" LEAD SCORING AI - AUTOMATED REAL-TIME + BATCH PROCESSING")
    print("=" * 70)
    
    print("\n1. CONNECTING TO SALESFORCE...")
    sf_integrator = SalesforceIntegrator()
    
    if not sf_integrator.sf:
        print(" Failed to connect to Salesforce. Exiting.")
        return
    
    print("\n2. INITIALIZING AI MODEL...")
    ai_model = LeadScoringAI()
    
    model_file = 'lead_scoring_model.pkl'
    if os.path.exists(model_file):
        ai_model.load_model(model_file)
        print("    Model loaded from file")
    else:
        print("    Training new model...")
        ai_model.train(n_samples=1000)
        ai_model.save_model(model_file)
        print("    Model trained and saved")
    
    print("\n3. STARTING REAL-TIME POLLING SERVICE...")
    event_subscriber = SalesforceEventSubscriber(sf_integrator)
    event_subscriber.set_ai_model(ai_model)
    
    # Start both services
    import threading
    
    # Thread for real-time polling
    def start_polling():
        asyncio.run(event_subscriber.start_polling(interval_seconds=30))
    
    # Start daily batch scheduler
    event_subscriber.schedule_daily_batch()
    
    polling_thread = threading.Thread(target=start_polling, daemon=True)
    polling_thread.start()
    
    print("\n4. SERVICES STARTED:")
    print("   ✓ Real-time lead processing (every 30 seconds)")
    print(f"   ✓ Daily batch processing (at {Config.DAILY_UPDATE_HOUR:02d}:00)")
    print("   ✓ Press Ctrl+C to stop all services")
    
    try:
        # Keep main thread alive
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nShutting down all services...")

def main_manual():
    """Original manual version (for testing)"""
    print("LEAD SCORING AI - MANUAL BATCH PROCESSING")
    print("=" * 70)
    
    sf_integrator = SalesforceIntegrator()
    real_leads_df = sf_integrator.fetch_leads_from_salesforce(limit=5)
    
    if real_leads_df.empty:
        print(" No real leads fetched from Salesforce. Using sample data instead.")
        sample_leads = [
            {
                'id': '00Q5e000001A1B2C',
                'email': 'lead1@example.com',
                'lead_source': 'Website',
                'company_size': 'Medium',
                'industry': 'Technology',
                'country': 'USA',  
                'website_visits': 15,
                'email_opens': 12,
                'time_on_site': 600,
                'pages_viewed': 20,
                'form_submissions': 3,
                'email_click_rate': 0.6,
                'days_since_created': 5,
                'has_demo_requested': 1,
                'content_downloads': 2
            }
        ]
        leads_to_analyze = sample_leads
    else:
        print(" Using real Salesforce leads for analysis!")
        leads_to_analyze = []
        for _, row in real_leads_df.iterrows():
            lead_data = {
                'Id': row['Id'],
                'Name': row['Name'],
                'CreatedDate': row['CreatedDate'],
                'lead_source': np.random.choice(['Website', 'Email', 'Social Media', 'Referral']),
                'company_size': np.random.choice(['Small', 'Medium', 'Large']),
                'industry': np.random.choice(['Technology', 'Finance', 'Healthcare']),
                'country': 'USA',
                'website_visits': np.random.randint(1, 20),
                'email_opens': np.random.randint(1, 15),
                'time_on_site': np.random.randint(100, 600),
                'pages_viewed': np.random.randint(5, 25),
                'form_submissions': np.random.randint(0, 4),
                'email_click_rate': np.random.uniform(0.1, 0.8),
                'days_since_created': np.random.randint(1, 30),
                'has_demo_requested': np.random.randint(0, 2),
                'content_downloads': np.random.randint(0, 3)
            }
            leads_to_analyze.append(lead_data)
    
    ai_model = LeadScoringAI()
    ai_model.train(n_samples=1000)
    results = ai_model.analyze_leads(leads_to_analyze)
    
    print("\nLEAD SCORING RESULTS:")
    print("=" * 80)
    for _, result in results.iterrows():
        print(f"Lead ID: {result['lead_id']}")
        print(f" Score: {result['score']}/100")
        print(f" Category: {result['category']}")
        print(f" Priority: {result['priority']}")
        print(f" Action: {result['recommendation']}")
        print("-" * 50)
    
    if sf_integrator.sf and not real_leads_df.empty:
        for _, row in results.iterrows():
            lead_id = row['lead_id']
            sf_integrator.update_lead(lead_id, row.to_dict())

def process_old_leads_manual():
    """Manual trigger for old lead processing"""
    print(" MANUAL OLD LEAD PROCESSING")
    print("=" * 50)
    
    sf_integrator = SalesforceIntegrator()
    ai_model = LeadScoringAI()
    
    model_file = 'lead_scoring_model.pkl'
    if os.path.exists(model_file):
        ai_model.load_model(model_file)
    else:
        ai_model.train(n_samples=1000)
        ai_model.save_model(model_file)
    
    event_subscriber = SalesforceEventSubscriber(sf_integrator)
    event_subscriber.set_ai_model(ai_model)
    
    count = event_subscriber.process_old_leads_batch()
    print(f"\nManual processing completed: {count} leads updated")

def main():
    """Main entry point - choose automated, manual, or batch mode"""
    if len(sys.argv) > 1:
        if sys.argv[1] == '--manual':
            main_manual()
        elif sys.argv[1] == '--batch':
            process_old_leads_manual()
        else:
            print("Usage:")
            print("  python script.py           # Automated mode (real-time + daily batch)")
            print("  python script.py --manual  # Manual batch processing")
            print("  python script.py --batch   # Process old leads only")
    else:
        main_automated()

if __name__ == "__main__":
    main()
